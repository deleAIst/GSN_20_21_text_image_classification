{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://debuggercafe.com/creating-efficient-image-data-loaders-in-pytorch-for-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfrom=transforms.Compose([transforms.Resize(224),\n",
    "                        transforms.CenterCrop(224),\n",
    "                             transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, labels, tfms=None):\n",
    "        # data loading\n",
    "        self.X = path\n",
    "        self.y = labels\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # dataset[0]\n",
    "        image = Image.open(self.X[index]).convert('RGB')\n",
    "        image = transfrom(image)\n",
    "        #image = np.transpose(image, (2, 0, 1))\n",
    "        type(image)\n",
    "        label = self.y[index]\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return(len(self.X))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model(DensNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_channels, num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(input_channels), nn.ReLU(),\n",
    "        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_convs, input_channels, num_channels):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        layer = []\n",
    "        for i in range(num_convs):\n",
    "            layer.append(conv_block(\n",
    "                num_channels * i + input_channels, num_channels))\n",
    "        self.net = nn.Sequential(*layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            # Concatenate the input and output of each block on the channel\n",
    "            # dimension\n",
    "            X = torch.cat((X, Y), dim=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_block(input_channels, num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(input_channels), nn.ReLU(),\n",
    "        nn.Conv2d(input_channels, num_channels, kernel_size=1),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(\n",
    "    nn.Conv2d(3, 224, kernel_size=7, stride=2, padding=3),\n",
    "    nn.BatchNorm2d(224), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels, growth_rate = 224, 32\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "blks = []\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    blks.append(DenseBlock(num_convs, num_channels, growth_rate))\n",
    "    # This is the number of output channels in the previous dense block\n",
    "    num_channels += num_convs * growth_rate\n",
    "    # A transition layer that haves the number of channels is added between\n",
    "    # the dense blocks\n",
    "    if i != len(num_convs_in_dense_blocks) - 1:\n",
    "        blks.append(transition_block(num_channels, num_channels // 2))\n",
    "        num_channels = num_channels // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "           torch.nn.init.xavier_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    b1, *blks,\n",
    "    nn.BatchNorm2d(num_channels), nn.ReLU(),\n",
    "    nn.AdaptiveMaxPool2d((1, 1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(num_channels, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create TensorBorad\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter(comment=\"test_old_Text_analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/data.csv')\n",
    "X_ = df.image_path.values\n",
    "y_ = df.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Splite data on train and test\n",
    "(xtrain, xtest, ytrain, ytest) = train_test_split(X_, y_,test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(xtrain, ytrain, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, lr_list, batch_size_list):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    for batch_size in batch_size_list:\n",
    "       for lr in lr_list:\n",
    "          net = nn.Sequential(\n",
    "               b1, *blks,\n",
    "               nn.BatchNorm2d(num_channels), nn.ReLU(),\n",
    "               nn.AdaptiveMaxPool2d((1, 1)),\n",
    "               nn.Flatten(),\n",
    "               nn.Linear(num_channels, 2))\n",
    "\n",
    "   \n",
    "          train_data = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "          writer = SummaryWriter(comment=f' batch_size={batch_size} lr={lr}')\n",
    "          optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "          net.apply(init_weights)\n",
    "          print('training on batch_size={batch_size}, lr={lr} and ', device)\n",
    "          net.to(device)\n",
    "          optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "          loss = nn.CrossEntropyLoss()\n",
    "          for epoch in range(num_epochs):\n",
    "             running_loss = 0.0\n",
    "             running_correct = 0\n",
    "        # Sum of training loss, sum of training accuracy, no. of examples\n",
    "             for i ,(X, y) in enumerate(train_data):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                y_hat = net(X)\n",
    "\n",
    "                l = loss(y_hat, y)\n",
    "                running_loss += l.item()*batch_size\n",
    "                running_correct += get_num_correct(y_hat, y)\n",
    "            \n",
    "                l.backward()\n",
    "                optimizer.step()\n",
    "             #fore Tensorborad\n",
    "            \n",
    "\n",
    "             writer.add_scalar('training loss',\n",
    "                     running_loss,\n",
    "                     epoch)\n",
    "             writer.add_scalar('training correct',\n",
    "                     running_correct,\n",
    "                     epoch)\n",
    "             # log the epoch accuracy\n",
    "             writer.add_scalar('training accuracy',\n",
    "                      running_correct/len(train_data),\n",
    "                      epoch)\n",
    "        \n",
    "             for name, weight in net.named_parameters():\n",
    "                writer.add_histogram(name,weight, epoch)\n",
    "                writer.add_histogram(f'{name}.grad',weight.grad, epoch)\n",
    "        \n",
    "             print(f\"Epoch {epoch+1} train loss: {running_loss/len(train_data):.3f} train acc: {running_correct/len(train_data)}\")\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on batch_size={batch_size}, lr={lr} and  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-6a56d1d4b421>:15: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  return image, torch.tensor(label, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 10.146 train acc: 3.2\n",
      "Epoch 2 train loss: 6.578 train acc: 3.4\n",
      "Epoch 3 train loss: 1.744 train acc: 4.2\n",
      "Epoch 4 train loss: 0.617 train acc: 4.5\n",
      "Epoch 5 train loss: 0.214 train acc: 4.7\n",
      "Epoch 6 train loss: 0.038 train acc: 4.8\n",
      "Epoch 7 train loss: 0.027 train acc: 4.8\n",
      "Epoch 8 train loss: 0.023 train acc: 4.8\n",
      "Epoch 9 train loss: 0.020 train acc: 4.8\n",
      "Epoch 10 train loss: 0.018 train acc: 4.8\n",
      "training on batch_size={batch_size}, lr={lr} and  cpu\n",
      "Epoch 1 train loss: 3.391 train acc: 3.7\n",
      "Epoch 2 train loss: 2.842 train acc: 3.6\n",
      "Epoch 3 train loss: 1.374 train acc: 4.1\n",
      "Epoch 4 train loss: 0.353 train acc: 4.8\n",
      "Epoch 5 train loss: 0.129 train acc: 4.8\n",
      "Epoch 6 train loss: 0.083 train acc: 4.8\n",
      "Epoch 7 train loss: 0.067 train acc: 4.8\n",
      "Epoch 8 train loss: 0.058 train acc: 4.8\n",
      "Epoch 9 train loss: 0.051 train acc: 4.8\n",
      "Epoch 10 train loss: 0.046 train acc: 4.8\n",
      "training on batch_size={batch_size}, lr={lr} and  cpu\n",
      "Epoch 1 train loss: 3.830 train acc: 3.7\n",
      "Epoch 2 train loss: 1.694 train acc: 4.0\n",
      "Epoch 3 train loss: 0.878 train acc: 4.6\n",
      "Epoch 4 train loss: 0.571 train acc: 4.6\n",
      "Epoch 5 train loss: 0.448 train acc: 4.8\n",
      "Epoch 6 train loss: 0.373 train acc: 4.8\n",
      "Epoch 7 train loss: 0.326 train acc: 4.8\n",
      "Epoch 8 train loss: 0.291 train acc: 4.8\n",
      "Epoch 9 train loss: 0.264 train acc: 4.8\n",
      "Epoch 10 train loss: 0.243 train acc: 4.8\n",
      "training on batch_size={batch_size}, lr={lr} and  cpu\n",
      "Epoch 1 train loss: 3.755 train acc: 2.9\n",
      "Epoch 2 train loss: 3.155 train acc: 3.0\n",
      "Epoch 3 train loss: 2.716 train acc: 3.5\n",
      "Epoch 4 train loss: 2.386 train acc: 3.8\n",
      "Epoch 5 train loss: 2.143 train acc: 4.1\n",
      "Epoch 6 train loss: 1.965 train acc: 4.1\n",
      "Epoch 7 train loss: 1.820 train acc: 4.1\n",
      "Epoch 8 train loss: 1.693 train acc: 4.2\n",
      "Epoch 9 train loss: 1.585 train acc: 4.4\n",
      "Epoch 10 train loss: 1.489 train acc: 4.4\n",
      "training on batch_size={batch_size}, lr={lr} and  cpu\n",
      "Epoch 1 train loss: 15.242 train acc: 5.4\n",
      "Epoch 2 train loss: 12.576 train acc: 6.8\n",
      "Epoch 3 train loss: 2.969 train acc: 8.8\n",
      "Epoch 4 train loss: 1.018 train acc: 9.4\n",
      "Epoch 5 train loss: 0.569 train acc: 9.6\n",
      "Epoch 6 train loss: 0.178 train acc: 9.6\n",
      "Epoch 7 train loss: 0.094 train acc: 9.6\n",
      "Epoch 8 train loss: 0.079 train acc: 9.6\n",
      "Epoch 9 train loss: 0.069 train acc: 9.6\n",
      "Epoch 10 train loss: 0.062 train acc: 9.6\n",
      "training on batch_size={batch_size}, lr={lr} and  cpu\n",
      "Epoch 1 train loss: 11.527 train acc: 5.2\n",
      "Epoch 2 train loss: 4.570 train acc: 7.6\n",
      "Epoch 3 train loss: 2.303 train acc: 9.2\n",
      "Epoch 4 train loss: 0.726 train acc: 9.6\n",
      "Epoch 5 train loss: 0.307 train acc: 9.6\n",
      "Epoch 6 train loss: 0.227 train acc: 9.6\n",
      "Epoch 7 train loss: 0.191 train acc: 9.6\n",
      "Epoch 8 train loss: 0.167 train acc: 9.6\n",
      "Epoch 9 train loss: 0.149 train acc: 9.6\n",
      "Epoch 10 train loss: 0.135 train acc: 9.6\n",
      "training on batch_size={batch_size}, lr={lr} and  cpu\n",
      "Epoch 1 train loss: 12.302 train acc: 7.6\n",
      "Epoch 2 train loss: 8.825 train acc: 7.6\n",
      "Epoch 3 train loss: 6.405 train acc: 7.6\n",
      "Epoch 4 train loss: 4.514 train acc: 7.6\n",
      "Epoch 5 train loss: 3.203 train acc: 7.8\n",
      "Epoch 6 train loss: 2.408 train acc: 8.2\n",
      "Epoch 7 train loss: 1.843 train acc: 8.8\n",
      "Epoch 8 train loss: 1.508 train acc: 9.0\n",
      "Epoch 9 train loss: 1.304 train acc: 9.2\n",
      "Epoch 10 train loss: 1.154 train acc: 9.4\n",
      "training on batch_size={batch_size}, lr={lr} and  cpu\n",
      "Epoch 1 train loss: 29.088 train acc: 2.0\n",
      "Epoch 2 train loss: 26.642 train acc: 2.0\n",
      "Epoch 3 train loss: 24.609 train acc: 2.0\n",
      "Epoch 4 train loss: 22.829 train acc: 2.0\n",
      "Epoch 5 train loss: 21.309 train acc: 2.0\n",
      "Epoch 6 train loss: 19.922 train acc: 2.0\n",
      "Epoch 7 train loss: 18.663 train acc: 2.0\n",
      "Epoch 8 train loss: 17.497 train acc: 2.0\n",
      "Epoch 9 train loss: 16.423 train acc: 2.0\n",
      "Epoch 10 train loss: 15.425 train acc: 2.0\n",
      "training on batch_size={batch_size}, lr={lr} and  cpu\n",
      "Epoch 1 train loss: 47.407 train acc: 10.0\n",
      "Epoch 2 train loss: 17.273 train acc: 11.666666666666666\n",
      "Epoch 3 train loss: 13.127 train acc: 12.0\n",
      "Epoch 4 train loss: 3.000 train acc: 15.666666666666666\n",
      "Epoch 5 train loss: 5.222 train acc: 13.666666666666666\n",
      "Epoch 6 train loss: 1.658 train acc: 15.0\n",
      "Epoch 7 train loss: 0.250 train acc: 16.0\n",
      "Epoch 8 train loss: 0.192 train acc: 16.0\n",
      "Epoch 9 train loss: 0.163 train acc: 16.0\n",
      "Epoch 10 train loss: 0.144 train acc: 16.0\n",
      "training on batch_size={batch_size}, lr={lr} and  cpu\n",
      "Epoch 1 train loss: 19.144 train acc: 8.333333333333334\n",
      "Epoch 2 train loss: 11.148 train acc: 12.333333333333334\n",
      "Epoch 3 train loss: 4.461 train acc: 14.666666666666666\n",
      "Epoch 4 train loss: 1.950 train acc: 15.666666666666666\n",
      "Epoch 5 train loss: 0.923 train acc: 16.0\n",
      "Epoch 6 train loss: 0.707 train acc: 16.0\n",
      "Epoch 7 train loss: 0.598 train acc: 16.0\n",
      "Epoch 8 train loss: 0.521 train acc: 16.0\n",
      "Epoch 9 train loss: 0.469 train acc: 16.0\n",
      "Epoch 10 train loss: 0.426 train acc: 16.0\n",
      "training on batch_size={batch_size}, lr={lr} and  cpu\n",
      "Epoch 1 train loss: 33.404 train acc: 12.666666666666666\n",
      "Epoch 2 train loss: 24.572 train acc: 12.666666666666666\n",
      "Epoch 3 train loss: 18.537 train acc: 12.666666666666666\n",
      "Epoch 4 train loss: 14.014 train acc: 12.666666666666666\n",
      "Epoch 5 train loss: 10.709 train acc: 12.666666666666666\n",
      "Epoch 6 train loss: 8.404 train acc: 13.333333333333334\n",
      "Epoch 7 train loss: 6.625 train acc: 13.333333333333334\n",
      "Epoch 8 train loss: 5.257 train acc: 13.333333333333334\n",
      "Epoch 9 train loss: 4.315 train acc: 13.333333333333334\n",
      "Epoch 10 train loss: 3.574 train acc: 13.666666666666666\n",
      "training on batch_size={batch_size}, lr={lr} and  cpu\n",
      "Epoch 1 train loss: 20.052 train acc: 7.0\n",
      "Epoch 2 train loss: 19.008 train acc: 7.0\n",
      "Epoch 3 train loss: 18.053 train acc: 7.0\n",
      "Epoch 4 train loss: 17.219 train acc: 7.0\n",
      "Epoch 5 train loss: 16.467 train acc: 7.666666666666667\n",
      "Epoch 6 train loss: 15.766 train acc: 7.666666666666667\n",
      "Epoch 7 train loss: 15.123 train acc: 7.666666666666667\n",
      "Epoch 8 train loss: 14.519 train acc: 8.0\n",
      "Epoch 9 train loss: 13.951 train acc: 8.333333333333334\n",
      "Epoch 10 train loss: 13.412 train acc: 9.333333333333334\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "batch_size_list = [5, 10, 20]\n",
    "lr_list = [.01, .001, .0001, .00001]\n",
    "num_epochs = 10\n",
    "train(num_epochs, lr_list, batch_size_list)\n",
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=20)\n",
    "net = nn.Sequential(\n",
    "               b1, *blks,\n",
    "               nn.BatchNorm2d(num_channels), nn.ReLU(),\n",
    "               nn.AdaptiveMaxPool2d((1, 1)),\n",
    "               nn.Flatten(),\n",
    "               nn.Linear(num_channels, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment=\"best_old_Text_analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-6a56d1d4b421>:15: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  return image, torch.tensor(label, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.765 train acc: 11.666666666666666\n",
      "Epoch 2 train loss: 0.331 train acc: 14.333333333333334\n",
      "Epoch 3 train loss: 0.147 train acc: 15.666666666666666\n",
      "Epoch 4 train loss: 0.061 train acc: 16.0\n",
      "Epoch 5 train loss: 0.040 train acc: 16.0\n",
      "Epoch 6 train loss: 0.033 train acc: 16.0\n",
      "Epoch 7 train loss: 0.028 train acc: 16.0\n",
      "Epoch 8 train loss: 0.025 train acc: 16.0\n",
      "Epoch 9 train loss: 0.023 train acc: 16.0\n",
      "Epoch 10 train loss: 0.021 train acc: 16.0\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "def train(net, train_iter, num_epochs, lr):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "           torch.nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "         running_loss = 0.0\n",
    "         running_correct = 0\n",
    "        # Sum of training loss, sum of training accuracy, no. of examples\n",
    "         for i ,(X, y) in enumerate(train_data):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = net(X)\n",
    "\n",
    "            l = loss(y_hat, y)\n",
    "            running_loss += l.item()\n",
    "            running_correct += get_num_correct(y_hat, y)\n",
    "            \n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            #fore Tensorborad\n",
    "            \n",
    "\n",
    "         writer.add_scalar('training loss',\n",
    "                    running_loss,\n",
    "                    epoch)\n",
    "         writer.add_scalar('training correct',\n",
    "                    running_correct,\n",
    "                    epoch)\n",
    "        # log the epoch accuracy\n",
    "         writer.add_scalar('training accuracy',\n",
    "                    running_correct/len(train_data),\n",
    "                    epoch)\n",
    "        \n",
    "         for name, weight in net.named_parameters():\n",
    "           writer.add_histogram(name,weight, epoch)\n",
    "           writer.add_histogram(f'{name}.grad',weight.grad, epoch)\n",
    "        \n",
    "         print(f\"Epoch {epoch+1} train loss: {running_loss/len(train_data):.3f} train acc: {running_correct/len(train_data)}\")\n",
    "    print('Finished Training')\n",
    "train(net, train_data, 10, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(y_true, y_predicted):\n",
    "    return confusion_matrix(y_true, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.get_cmap('Blues')): \n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    fig =plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(ticks=range(cm.min(), cm.max() + 1, int((cm.max() - cm.min()) / 10)))\n",
    "\n",
    "    target_names = ['0', '1']\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True value')\n",
    "    plt.xlabel('Predicted value\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "    writer.add_figure(title, fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(xtrain, ytrain, 0)\n",
    "train_data = torch.utils.data.DataLoader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-6a56d1d4b421>:15: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  return image, torch.tensor(label, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "#X = []\n",
    "#y = []\n",
    "net.to('cpu')\n",
    "y_true = []\n",
    "y_predicted = []\n",
    "softmax = nn.Softmax(dim=1)\n",
    "for (image, label) in train_data:\n",
    "    y_true.append(label.detach())\n",
    "    y_predicted_tensor = net(image)\n",
    "    y_predicted_tensor = softmax(y_predicted_tensor)\n",
    "    y_predicted.append(torch.max(y_predicted_tensor, 1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = create_confusion_matrix(y_true, y_predicted)\n",
    "plot_confusion_matrix(cm, 'Confusion matrix train data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(xtest, ytest, 0)\n",
    "train_data = torch.utils.data.DataLoader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.to('cpu')\n",
    "y_true = []\n",
    "y_predicted = []\n",
    "softmax = nn.Softmax(dim=1)\n",
    "for (image, label) in train_data:\n",
    "    y_true.append(label.detach())\n",
    "    y_predicted_tensor = net(image)\n",
    "    y_predicted_tensor = softmax(y_predicted_tensor)\n",
    "    y_predicted.append(torch.max(y_predicted_tensor, 1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = create_confusion_matrix(y_true, y_predicted)\n",
    "plot_confusion_matrix(cm, 'Confusion matrix test data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_data)\n",
    "images, labels = dataiter.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('old_text_image_images', img_grid)\n",
    "net.to('cpu')\n",
    "writer.add_graph(net, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
